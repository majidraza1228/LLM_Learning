{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majidraza1228/LLM/blob/LLM_Tech_Class/mpt_7b_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPdQvYmlWmNc"
      },
      "source": [
        "# MPT-7B-chat\n",
        "\n",
        "---\n",
        "\n",
        "ðŸš¨ **Note: this must be run on a GPU. If you run this on a CPU, even a very fast one, it can take many hours to answer a single question!**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQeerDFLNj1V"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torchaudio torchvision torch -y"
      ],
      "metadata": {
        "id": "_iDT2JbARP0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install torchaudio==2.5.0"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "h6FTs8ymU0Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_fRq0BSGMBk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate einops langchain xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikzdi_uMI7B-"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mosaicml/mpt-7b-chat\",\n",
        "                                          trust_remote_code=True)\n",
        "\n",
        "config={\"init_device\": \"meta\"}\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b-chat\",\n",
        "                                             trust_remote_code=True,\n",
        "                                             config=config,\n",
        "                                             torch_dtype=bfloat16)\n",
        "\n",
        "print(f\"device={device}\")\n",
        "print('model loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUHC7VgXw-BC"
      },
      "outputs": [],
      "source": [
        "# Use the GPU\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xOAkewecrOVP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython.display import Markdown\n",
        "import torch\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# mtp-7b is trained to add \"<|endoftext|>\" at the end of generations\n",
        "stop_token_ids = [tokenizer.eos_token_id]\n",
        "\n",
        "# define custom stopping criteria object.\n",
        "# Source: https://github.com/pinecone-io/examples/blob/master/generation/llm-field-guide/mpt-7b/mpt-7b-huggingface-langchain.ipynb\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor,scores: torch.FloatTensor,\n",
        "                 **kwargs) -> bool:\n",
        "        for stop_id in stop_token_ids:\n",
        "            if input_ids[0][-1] == stop_id:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
        "\n",
        "def ask_question(question, max_length=100):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Encode the question\n",
        "    input_ids = tokenizer.encode(question, return_tensors='pt')\n",
        "\n",
        "    # Use the GPU\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    # Generate a response\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_length,\n",
        "        temperature=0.9,\n",
        "        stopping_criteria=stopping_criteria\n",
        "    )\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(output[:, input_ids.shape[-1]:][0],\n",
        "                                skip_special_tokens=True)\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    display(Markdown(response))\n",
        "\n",
        "    print(\"Function duration:\", duration, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4s3LlRtijSuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "outputId": "096ebf0b-dc66-4bee-f8b1-0bd972a19c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nimport csv\nimport reportlab.pdfgen\n\n# open the csv file\nwith open('data.csv', 'r') as csvfile:\n    csvreader = csv.reader(csvfile)\n    \n    # loop through the csv file\n    for row in csvreader:\n        # create a new PDF document\n        pdf = reportlab.pdfgen.PDFGenerator()\n        \n        # add a page to the PDF document\n        pdf.addPage()\n        \n        # loop through the row\n        for col in row:\n            # add text to the PDF document\n            pdf.setFont('Helvetica', 12)\n            pdf.drawString(100, 750, col)\n        \n        # save the PDF document\n        pdf.save('output.pdf')\n        \n        # add a blank line to the PDF document\n        pdf.setFont('Helvetica', 12)\n        pdf.drawString(100, 750, '')\n        \n        # move the PDF writer to the next page\n        pdf.setPage(pdf.getNumPages() + 1)\n```\n\nThis code reads in a CSV file, creates a new PDF document, and loops through each row in the CSV file, adding text to the PDF document for each column in the row. It then saves the PDF document and moves on to the next row. The resulting PDF document will have one page per row in the CSV file.\n\nTo use this code, you will need to have the reportlab library installed. You can install it using pip:\n\n```\npip install reportlab\n```\n\nYou will also need to have a CSV file with the appropriate data. You can create a CSV file using a text editor or spreadsheet program, such as Microsoft Excel.\n\nOnce you have a CSV file and the reportlab library installed, you can run the Python"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function duration: 510.75560235977173 seconds\n"
          ]
        }
      ],
      "source": [
        "# Ask a question\n",
  
        "#ask_question(\"What is the capital of France?\", 200)\n",
        "ask_question(\"write python code that converts a csv into a pdf\", 400)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
