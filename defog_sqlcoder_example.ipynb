{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majidraza1228/LLM/blob/LLM_Tech_Class/defog_sqlcoder_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SQLCoder-7b-2\n",
        "Run the cells below to run inference on our text-to-SQL LLM: SQLCoder-7b-2.\n",
        "\n",
        "⭐️ [Github Repo](https://github.com/defog-ai/sqlcoder)\n",
        "\n",
        "🤗 [Huggingface Page](https://huggingface.co/defog/sqlcoder-7b-2)"
      ],
      "metadata": {
        "id": "Hnhvqa6ZRscb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "ZKYJQvVmSEWs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bAMjQKJfG3d"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers bitsandbytes accelerate sqlparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "BzllQomZfQnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hztT0MXkfRs1",
        "outputId": "42a64f17-4d44-4a67-e973-c65b1efc20cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "available_memory = torch.cuda.get_device_properties(0).total_memory"
      ],
      "metadata": {
        "id": "7jsl838clW_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(available_memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTQnCfsen16q",
        "outputId": "fee79b57-de7a-414d-f592-807fe912afe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42481811456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download the Model\n",
        "Use any model on Colab (or any system with >30GB VRAM on your own machine) to load this in f16. If unavailable, use a GPU with minimum 8GB VRAM to load this in 8bit, or with minimum 5GB of VRAM to load in 4bit.\n",
        "\n",
        "This step can take around 5 minutes the first time. So please be patient :)"
      ],
      "metadata": {
        "id": "Tt4ilTkpTMoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"defog/sqlcoder-7b-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if available_memory > 15e9:\n",
        "    # if you have atleast 15GB of GPU memory, run load the model in float16\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        use_cache=True,\n",
        "    )\n",
        "else:\n",
        "    # else, load in 8 bits – this is a bit slower\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        # torch_dtype=torch.float16,\n",
        "        load_in_8bit=True,\n",
        "        device_map=\"auto\",\n",
        "        use_cache=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "_mNT25UOfSMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Set the Question & Prompt and Tokenize\n",
        "Feel free to change the schema in the prompt below to your own schema"
      ],
      "metadata": {
        "id": "TzLcpXyzTkHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"### Task\n",
        "Generate a SQL query to answer [QUESTION]{question}[/QUESTION]\n",
        "\n",
        "### Instructions\n",
        "- If you cannot answer the question with the available database schema, return 'I do not know'\n",
        "- Remember that revenue is price multiplied by quantity\n",
        "- Remember that cost is supply_price multiplied by quantity\n",
        "\n",
        "### Database Schema\n",
        "This query will run on a database whose schema is represented in this string:\n",
        "CREATE TABLE products (\n",
        "  product_id INTEGER PRIMARY KEY, -- Unique ID for each product\n",
        "  name VARCHAR(50), -- Name of the product\n",
        "  price DECIMAL(10,2), -- Price of each unit of the product\n",
        "  quantity INTEGER  -- Current quantity in stock\n",
        ");\n",
        "\n",
        "CREATE TABLE customers (\n",
        "   customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n",
        "   name VARCHAR(50), -- Name of the customer\n",
        "   address VARCHAR(100) -- Mailing address of the customer\n",
        ");\n",
        "\n",
        "CREATE TABLE departments ( department_id INTEGER PRIMARY KEY, -- Unique ID for each department name VARCHAR(50) -- Name of the department );\n",
        "\n",
        "CREATE TABLE projects ( project_id INTEGER PRIMARY KEY, -- Unique ID for each project name VARCHAR(50), -- Name of the project budget DECIMAL(10,2) -- Budget allocated for the project );\n",
        "\n",
        "CREATE TABLE project_assignments ( assignment_id INTEGER PRIMARY KEY, -- Unique ID for each assignment project_id INTEGER, -- ID of the project employee_id INTEGER, -- ID of the employee assigned to the project hours_worked INTEGER -- Number of hours worked on the project by the employee );\n",
        "\n",
        "-- project_assignments.project_id can be joined with projects.project_id -- project_assignments.employee_id can be joined with employees.employee_id\n",
        "\n",
        "### Answer\n",
        "Given the database schema, here is the SQL query that answers [QUESTION]{question}[/QUESTION]\n",
        "[SQL]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0eI-VpCkf-fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate the SQL\n",
        "This can be excruciatingly slow on a T4 in Colab, and can take 10-20 seconds per query. On faster GPUs, this will take ~1-2 seconds\n",
        "\n",
        "Ideally, you should use `num_beams`=4 for best results. But because of memory constraints, we will stick to just 1 for now."
      ],
      "metadata": {
        "id": "I2Lyi9VcVoTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlparse\n",
        "\n",
        "def generate_query(question):\n",
        "    updated_prompt = prompt.format(question=question)\n",
        "    inputs = tokenizer(updated_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        max_new_tokens=400,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "    )\n",
        "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    # empty cache so that you do generate more results w/o memory crashing\n",
        "    # particularly important on Colab – memory management is much more straightforward\n",
        "    # when running on an inference service\n",
        "    return sqlparse.format(outputs[0].split(\"[SQL]\")[-1], reindent=True)"
      ],
      "metadata": {
        "id": "4iBKg6Rmmpnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the current stock quantity for each product?\"\n",
        "generated_sql = generate_query(question)"
      ],
      "metadata": {
        "id": "jCJvcurqlP2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_sql)"
      ],
      "metadata": {
        "id": "flEJuo1OpYQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "09ysYJbsnKUh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}