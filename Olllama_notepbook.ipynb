{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyObnQwC/jS96JnjG3HlB59A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majidraza1228/LLM/blob/LLM_Tech_Class/Olllama_notepbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ej10y8XDwUTb"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n"
      ],
      "metadata": {
        "id": "C7Lf86A6w0Hc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!ollama pull llama3.1:8b\n",
        "clear_output()\n",
        "\n",
        "!pip install -U lightrag[ollama]\n",
        "\n",
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient, GroqAPIClient\n",
        "\n",
        "import time\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gXVD3_J-w5tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_template = r\"\"\"<SYS>\n",
        "You are a helpful assistant.\n",
        "</SYS>\n",
        "User: {{input_str}}\n",
        "You:\"\"\"\n"
      ],
      "metadata": {
        "id": "8McDPQyVxHoK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleQA(Component):\n",
        "    def __init__(self, model_client: ModelClient, model_kwargs: dict):\n",
        "        super().__init__()\n",
        "        self.generator = Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=qa_template,\n",
        "        )\n",
        "\n",
        "    def call(self, input: dict) -> str:\n",
        "        return self.generator.call({\"input_str\": str(input)})\n",
        "\n",
        "    async def acall(self, input: dict) -> str:\n",
        "        return await self.generator.acall({\"input_str\": str(input)})\n",
        "\n",
        "from lightrag.components.model_client import OllamaClient\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "model = {\n",
        "    \"model_client\": OllamaClient(),\n",
        "    \"model_kwargs\": {\"model\": \"llama3.1:8b\"}\n",
        "}\n",
        "qa = SimpleQA(**model)\n",
        "output=qa(\"where is Monroe NJ\")\n",
        "display(Markdown(f\"**Answer:** {output.data}\"))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ky5SJ8K4xSYN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}